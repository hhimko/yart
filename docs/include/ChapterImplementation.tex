%--------------------------------------------------------------------------------------------
%   YART thesis "Implementation" chapter definition.
%--------------------------------------------------------------------------------------------

\chapter{Implementation} \label{ch:Implementation}

Before delving into the implementation of a ray tracing engine, certain considerations should be made, which will define the entire development workflow.
One such example is the choice of a coordinate system, that will affect the order of mathematical calculations.
Coordinate systems provide a standardized way of specifying locations and orientations of objects within a multi-dimensional environment.
In the context of two-dimensional computer graphics, the system typically consists of a X-axis pointing from left to right, and a Y-axis pointing upwards.
When adding the third coordinate however, the developer has a choice as to whether the Z-axis should point out of, or into the virtual screen, away from the viewer.
These two system types are respectively referred to as the right-handed coordinate system (RHS), and the left-handed coordinate system (LHS) (see \cref{fig:Implementation/coordinate_system}).
The names of these systems derive from the \textit{right-hand rule}, which is a convention used for determining the orientation of axes in three-dimensional space \supercite{RightHandRule}. 

\vfill
\begin{figure}[!ht]
    \centering

    \begin{subfigure}{.4\textwidth}
        \centering

        \begin{tikzpicture}[scale=1.15, every node/.style={scale=1.15}]
            \draw[-{Latex[length=3mm]}, color=axis_red, very thick]   (0, 0) -- (2, -0.7);
            \draw[-{Latex[length=3mm]}, color=axis_green, very thick] (0, 0) -- (0, 2);
            \draw[-{Latex[length=3mm]}, color=axis_blue, very thick]  (0, 0) -- (-1.6, -0.9);
            \node[color=axis_red] (x) at (2.05, -0.4) {\textbf{x}};
            \node[color=axis_green] (y) at (-0.3, 1.95) {\textbf{y}};
            \node[color=axis_blue] (y) at (-1.7, -0.6) {\textbf{z}};
        \end{tikzpicture}
        \caption{}
    \end{subfigure}%
    \begin{subfigure}{.4\textwidth}
        \centering
        
        \begin{tikzpicture}[scale=1.25, every node/.style={scale=1.25}]
            \draw[-{Latex[length=3mm]}, color=axis_red, very thick]   (0, 0) -- (2, -0.7);
            \draw[-{Latex[length=3mm]}, color=axis_green, very thick] (0, 0) -- (0, 2);
            \draw[-{Latex[length=3mm]}, color=axis_blue, very thick]  (0, 0) -- (1.6, 0.9);
            \node[color=axis_red] (x) at (2.05, -0.4) {\textbf{x}};
            \node[color=axis_green] (y) at (-0.3, 1.95) {\textbf{y}};
            \node[color=axis_blue] (y) at (1.5, 1.12) {\textbf{z}};
        \end{tikzpicture}
        \caption{}
    \end{subfigure}

    \caption[RHS and LHS coordinate systems]{RHS (a) and LHS (b) coordinate systems.}
    \label{fig:Implementation/coordinate_system}
\end{figure}
\vfill

Both the RHS and LHS are commonly utilized in commercial rendering software.
While developing YART, the left-handed coordinate system has been used, solely based on personal preference. 
This information is important to keep in mind, as the choice of a system determines the definition and implementation of projections, and various matrix operations, used throughout a ray tracing engine. 

\section{Ray Definition}

The fundamental piece of any ray tracing engine is undeniably the concept of \textit{rays}.
In mathematics, a ray can be defined as a straight line extending infinitely in one direction from a specific starting point. 
It is therefore characterized by its origin point, and a unit vector denoting the directions it's facing.
Rays can be conceptualized across any number of dimensions, however for the specific application of ray tracing, we will only focus on three-dimensional rays.

Let's consider a ray with an origin point $ \bm{P} $, extending infinitely in the direction of a vector $ \bm{\hat{v}} $.
To sample a specific point along this ray, we define a function $ \bm{r} $
%
\begin{equation}
    \bm{r}(t) = \bm{P} + t\bm{\hat{v}}
\end{equation}
%
It returns all possible points on the ray, which are $ t $ units apart from its origin.
Knowing the distance from the ray's origin to the closest object hit along its path, function $ \bm{r} $ will enable us to find the exact point of collision on the object's surface.
For this purpose, it's important to make the direction vector $ \bm{\hat{v}} $ an unit vector, as a vector of length different than $ 1 $ would scale the distance proportionately, ultimately resulting in inaccurate calculations. 

In the code, we can define a ray as a lightweight structure containing two three-dimensional vector members, representing the ray's origin and direction (see \cref{lst:Implementation/RayDefinition/ray}).
This structure can later be extended with related operations, such as testing for ray-object intersections or reflecting of a given surface.

\vfill
\lstinputlisting[
    xleftmargin=1.5em, 
    caption={[Ray structure definition]
        Ray structure definition.},
    label={lst:Implementation/RayDefinition/ray}
]{include/listings/ListingRay.cpp}
\vfill

\section{Ray Generation and The Camera}

To render images using ray tracing methodology, generally means to trace a single ray for every pixel in the output image.
The color seen it the direction of those rays, is what defines the final color of their respective pixels. 
This initial phase of calculating ray directions for every image pixel, is often referred to as the \textit{ray generation} step of a ray tracing engine.
Ray generation in YART can be divided into four stages:
\begin{enumerate}
    \item defining and querying camera properties,
    \item computing a screen-space-to-camera-space transformation matrix,
    \item calculating individual ray directions using the matrix,
    \item caching the calculated direction vectors.
\end{enumerate}

\subsection{Camera Definition}

Before we can generate a ray, we first need to explain the basic concept behind a \textit{camera}.
Sometimes referred to as the \textit{eye}, a camera is the engine's virtual viewpoint, giving the ability to "see" into a digital environment.
At its core, it's used to generate rays for each pixel in the rendered image.
It is primarily represented by its position in world-space, a viewing direction (or \textit{look-at vector}), and the camera's field of view (FOV).
A camera's field of view represents the angular extent of the scene captured in the output image. 
FOV might be expressed in one of three different ways, as it can be measured horizontally, vertically, or diagonally.
The difference between them is significant, impacting the calculations made while defining a projection matrix.
In YART, horizontal measurement is used to determine camera's field of view.

The camera can be though of as a viewing context for rendering a scene.
Rays will be cast originating from the camera's position and traced in the direction it is facing, offset by \textit{screen-space} coordinates of subsequent image pixels.
Screen-space coordinates generally refer to a two-dimensional coordinate system that represents the positions of pixels on a screen.
In the context of ray tracing, the screen can be represented as an image, positioned at a specific distance from the camera's origin. 
This concept is otherwise knows as the camera's \textit{image plane} or \textit{viewport}.
The image plane pixel positions in camera's local-space (or \textit{camera-space}) is what will let us determine ray directions for a particular pixel. 
\cref{fig:Implementation/RayGeneration/iamge_plane} illustrates a camera with the center of its viewport positioned at the end of the look-at-vector.  

\vfill
\begin{figure}[!ht]
    \centering

    % Top margin
    \vspace{1cm}

    \begin{tikzpicture}[scale=1.0, every node/.style={scale=1.0}]
        % Plane inner lines
        \draw[-, color=gray, thin] (6.25, 2.42) -- (9.55, 1.6);
        \draw[-, color=gray, thin] (6.25, 1.9) -- (9.46, 0.78);
        \draw[-, color=gray, thin] (6.25, 1.38) -- (9.4, -0.05);
        \draw[-, color=gray, thin] (6.25, 0.9) -- (9.35, -0.85);
        \draw[-, color=gray, thin] (6.25, 0.4) -- (9.3, -1.62);

        \draw[-, color=gray, thin] (6.6, 2.92) -- (6.57, -0.35);
        \draw[-, color=gray, thin] (7, 2.87) -- (6.95, -0.63);
        \draw[-, color=gray, thin] (7.5, 2.82) -- (7.4, -0.98);
        \draw[-, color=gray, thin] (8.08, 2.7) -- (7.92, -1.35);
        \draw[-, color=gray, thin] (8.75, 2.65) -- (8.53, -1.82);

        % Pixel diagonals 
        \draw[-, color=gray, ultra thin] (7.95, -0.72) -- (8.6, -0.43);

        % Plane outer lines
        \draw[-, color=darkgray, very thick] (6.25, 2.97) -- (9.6, 2.53)
        -- (9.25, -2.35) -- (6.25, -0.1) -- (6.25, 2.97) -- (9.6, 2.53);
        \node[color=darkgray, rotate=-7] (x) at (7.9, 3.1) {\textbf{Image Plane}};

        % Looking direction vector
        \draw[-{Latex[length=3mm]}, color=axis_blue, very thick] (0.037, 0.005) -- (7.45, 0.835);
        \node[color=axis_blue, rotate=6.6] (x) at (4.55, 0.75) {\textbf{Look-at Vector}};

        % Ray 
        \draw[-{Latex[length=3mm]}, color=axis_red, very thick] (0.037, 0.005) -- (12, -0.855);
        \node[color=axis_red, rotate=-4] (x) at (11, -0.5) {\textbf{Ray}};

        % Camera to plane lines
        \draw[-, color=darkgray, thin] (0, 0) -- (6.235, 2.982);
        \draw[-, color=darkgray, thin] (0, 0) -- (9.6, 2.53);
        \draw[-, color=darkgray, thin] (0, 0) -- (9.25, -2.366);
        \draw[-, color=darkgray, thin] (0, 0) -- (6.25, -0.11);

        % Overlays over the ray
        \draw[{Round Cap[]}-{Round Cap[]}, color=gray, thin] (8.883, -0.5862) -- (9.15, -0.737);
        \draw[{Round Cap[]}-{Round Cap[]}, color=gray, thin] (8.593, -0.54) -- (8.583, -0.737);
        \draw[-, color=darkgray, very thick] (9.6, 2.53) -- (9.25, -2.35);

        % Pixel diagonals 
        \draw[-, color=gray, ultra thin] (7.973, -0.075) -- (8.57, -1.143);

        % Camera origin
        \node[fill=darkgray, circle, inner sep=0pt, minimum size=2.5mm] (c) at (0.95mm, 0) {};
        \node[color=darkgray, align=center] (x) at (-0.9, 0.25) {\textbf{Camera} \\ \textbf{Origin}};

    \end{tikzpicture}

    % Bottom margin
    \vspace{1cm}

    \caption[Visualization of a camera's image plane]{
        \centering
        Visualization of a camera's image plane, with a ray extending through a single pixel. Squares in the plane symbolize individual pixels in the rendered image. 
    }
    \label{fig:Implementation/RayGeneration/iamge_plane}
\end{figure}
\vfill

In a class defining a simplified camera model, we can introduce the functionality of calculating ray directions with a dedicated \verb|Camera::GetRayDirections| method. 
This method, can be declared to accept (as a parameter) an array of vectors, which will be populated with subsequent ray directions for every pixel in the output image.
Consequently, additional \verb|width| and \verb|height| parameters should be defined, which will specify the dimensions of the image, as well as the array's size.
\cref{lst:Implementation/RayGeneration/camera} demonstrates an example \verb|Camera| class declaration, constructed basing on the described methodology.

\vfill
\begin{figure}
    \lstinputlisting[
        xleftmargin=2em, 
        caption={[Basic \texttt{Camera} class declaration]
            Basic \texttt{Camera} class declaration.},
        label={lst:Implementation/RayGeneration/camera}
    ]{include/listings/ListingCamera1.cpp}
\end{figure}
\clearpage

\subsection{Transformation Matrices}

In order to convert individual screen-space pixel coordinates to ray directions, we'll first need to transform them into the world-space.
A common solution to this problem in computer graphics is to use an \textit{inverse view-projection matrix}, used for transforming vertices in screen-space into the world-space.
Usually denoted as $ (\bm{VP})^{-1} $, it is defined as the mathematical inverse of combined view and projection matrices.
Let's consider a point $ \bm{C}_{\textnormal{\small screen}} $ as a certain coordinate in screen-space.
The transformation to obtain a corresponding point $ \bm{C}_{\textnormal{\small world}} $ in world-space can be expressed as:
%
\begin{equation}
    \bm{C}_{\textnormal{\small world}} = (\bm{VP})^{-1} \cdot \bm{C}_{\textnormal{\small screen}} = (\bm{P}^{-1} \bm{V}^{-1}) \cdot \bm{C}_{\textnormal{\small screen}}
\end{equation}

To calculate an inverse view-projection matrix, we first need to construct a view matrix, which defines the intermediate step of transforming world-space coordinates into the camera's local space.
View matrices are often used in rasterization engines, to represent scene geometry relatively to a camera's position and orientation.
It can therefore be built using four vectors: the camera's world-space $ position $, along with three unit vectors, $ right $, $ up $, and $ forward $, which represent the camera's orientation:
%
\begin{equation}
    \bm{V} =
    \begin{bmatrix}
        right_x & up_x & forward_x & position_x\\
        right_y & up_y & forward_y & position_y\\
        right_z & up_z & forward_z & position_z\\
        0       & 0    & 0         & 1
    \end{bmatrix}
    \label{eq:Implementation/RayGeneration/view_matrix}
\end{equation}
%
However, since our goal is to only calculate ray directions, we can assume the camera is positioned at the world's origin, and just focus on its orientation.
Knowing the $ forward $ (looking) direction, the $ right $ and $ up $ vectors can be calculated in relation to the world's up (positive y-axis) direction, using vector cross products:  
%
{
    \newcommand*{\worldup}{\begin{bmatrix} 0 & 1 & 0 \end{bmatrix}\tran}
    \begin{align}
        right &= - \frac{forward\times\worldup}{\norm{forward\times\worldup}}\\[0.5em]
        up    &= - right \times forward
    \end{align}
    
}

Above equations aid us in implementing a view matrix creation function, seen in \cref{lst:Implementation/RayGeneration/view_matrix}.
Notice how the $ up $ vector gets negated in the function, as opposed to the general definition in \cref{eq:Implementation/RayGeneration/view_matrix}.
This is because ray directions are flipped on the y-axis in relation to screen-space pixel coordinates.
In YART, the $ y $ component of pixel coordinates increase when moving from top to bottom, whereas the ray pitch rotation should instead decrease.
The matrix returned from this function can later be inverted, ultimately resulting in the desired view matrix inverse.

\clearpage
\begin{figure}[!ht]
    \lstinputlisting[
        aboveskip=1\bigskipamount,
        xleftmargin=2em, 
        caption={[Implementation of a view matrix creation function]
            Implementation of a view matrix creation function.},
        label={lst:Implementation/RayGeneration/view_matrix}
    ]{include/listings/ListingViewMatrix.cpp}
\end{figure}

The next step in calculating a view-projection inverse is the definition of an \textit{inverse projection matrix}.
It is used in 3D computer graphics for transforming screen-space coordinates into camera-space.
Inverse projection matrices can be defined in numerous ways depending on the system's requirements.
In YART, this matrix is responsible for normalizing screen coordinates and centering them on the \textit{near clipping plane}. 
The near clipping plane is a mathematical plane placed in front of the camera, which determines how much of the scene should be \textit{clipped}, or hidden for rendering.
Given the output image dimensions $ (w, h) $ , and the near plane's distance $ d $, the inverse projection matrix $ \bm{P}^{-1} $ can be defined as follows:
%
\begin{equation}
    \newcommand*{\s}{\hspace{0.7em}}
    \bm{P}^{-1} =
    \begin{bmatrix}
        2u / w & 0      & \s 0 \s & \s 0 \s \\
        0      & 2v / h & 0       & 0       \\
        -u     & -v     & d       & 0       \\
        0      & 0      & 0       & 1
    \end{bmatrix},
    \label{eq:Implementation/RayGeneration/inverse_projection_matrix}
\end{equation}
%
where $ (u, v) $ are half the dimensions of the clipping plane, for a specified horizontal camera field of view in radians $ fov $:
%
\begin{align}
    u &= d \cdot \tan(fov / 2),\label{eq:Implementation/RayGeneration/u}\\[0.5em]
    v &= u \cdot h/w \label{eq:Implementation/RayGeneration/v}
\end{align}

\cref{lst:Implementation/RayGeneration/ip_matrix} demonstrates an example inverse projection matrix creation function based on Equations (\ref{eq:Implementation/RayGeneration/inverse_projection_matrix}\textendash\ref{eq:Implementation/RayGeneration/v}).

\begin{figure}[!ht]
    \lstinputlisting[
        xleftmargin=2em, 
        caption={[Implementation of an inverse projection matrix creation function]
            Implementation of an inverse projection matrix creation function.},
        label={lst:Implementation/RayGeneration/ip_matrix}
    ]{include/listings/ListingInverseProjectionMatrix.cpp}
\end{figure}


\subsection{Calculating Ray Directions}

\subsection{Ray Direction Caching}

YART employs a caching mechanism

Given the output image's pixel coordinates, the 

\section{Scene Representation}

mesh definition, normals calculation, uv coordinates, sphere mesh generation(?), light objects

\dots

\section{Intersection Testing, Materials, and Shading}

Möller-Trumbore algorithm for tri-ray intersection, solid color materials, shading using blinn-phong reflection model

\dots

\section{Sky Color Sampling}

solid color skies, linear gradients, and cubemap skyboxes

\dots

\section{Shadows}

hard shadows implementation

\dots

\section{Bounding Volume Hierarchies}

optimization strategies using BVH trees, AABB tree implementation

\dots
